import os
import aiohttp
import asyncio
from datetime import datetime, timedelta
import gspread
from time import time
from calendar import monthrange
import logging
from dotenv import load_dotenv
import json
import pandas as pd

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()

def load_api_tokens():
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–∞ tokens.json –≤–æ –≤—Å–µ—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è—Ö –ø—Ä–æ–µ–∫—Ç–∞
    current_dir = os.path.dirname(os.path.abspath(__file__))

    while True:
        tokens_path = os.path.join(current_dir, 'tokens.json')
        if os.path.isfile(tokens_path):
            try:
                with open(tokens_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except json.JSONDecodeError:
                print(f"–û—à–∏–±–∫–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è JSON –≤ —Ñ–∞–π–ª–µ: {tokens_path}")
                return None

        # –ü–æ–¥–Ω–∏–º–∞–µ–º—Å—è –Ω–∞ —É—Ä–æ–≤–µ–Ω—å –≤—ã—à–µ
        parent_dir = os.path.dirname(current_dir)
        if parent_dir == current_dir:
            # –î–æ—Å—Ç–∏–≥–ª–∏ –∫–æ—Ä–Ω—è –¥–∏—Å–∫–∞
            break
        current_dir = parent_dir

    print("–§–∞–π–ª tokens.json –Ω–µ –Ω–∞–π–¥–µ–Ω –Ω–∏ –≤ –æ–¥–Ω–æ–π –∏–∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π")
    return None

def safe_open_spreadsheet(title, retries=5, delay=5):
    """
    –ü—ã—Ç–∞–µ—Ç—Å—è –æ—Ç–∫—Ä—ã—Ç—å —Ç–∞–±–ª–∏—Ü—É —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏ –ø—Ä–∏ APIError 503.
    """
    gc = gspread.service_account(filename=os.path.join(os.path.dirname(__file__), 'creds.json'))
    
    for attempt in range(1, retries + 1):
        logging.info(f"[–ü–æ–ø—ã—Ç–∫–∞ {attempt}] –æ—Ç–∫—Ä—ã—Ç—å –¥–æ—Å—Ç—É–ø –∫ —Ç–∞–±–ª–∏—Ü–µ '{title}'")
        
        try:
            spreadsheet = gc.open(title)
            logging.info(f"‚úÖ –¢–∞–±–ª–∏—Ü–∞ '{title}' —É—Å–ø–µ—à–Ω–æ –æ—Ç–∫—Ä—ã—Ç–∞")
            return spreadsheet
            
        except gspread.APIError as e:
            error_code = e.response.status_code if hasattr(e, 'response') else None
            logging.info(f"‚ö†Ô∏è [–ü–æ–ø—ã—Ç–∫–∞ {attempt}/{retries}] APIError {error_code}: {e}")
            
            if error_code == 503:
                if attempt < retries:
                    logging.info(f"‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ {delay} —Å–µ–∫—É–Ω–¥ –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–æ–π...")
                    time.sleep(delay)
                    # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–π –ø–æ–ø—ã—Ç–∫–∏ (exponential backoff)
                    delay *= 2
                else:
                    logging.error("‚ùå –í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã")
                    raise
            else:
                # –î—Ä—É–≥–∏–µ –æ—à–∏–±–∫–∏ API (403, 404 –∏ —Ç.–¥.) - –Ω–µ –ø–æ–≤—Ç–æ—Ä—è–µ–º
                raise
                
        except gspread.SpreadsheetNotFound:
            logging.info(f"‚ùå –¢–∞–±–ª–∏—Ü–∞ '{title}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            raise
            
        except Exception as e:
            logging.error(f"‚ö†Ô∏è [–ü–æ–ø—ã—Ç–∫–∞ {attempt}/{retries}] –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")
            if attempt < retries:
                logging.error(f"‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ {delay} —Å–µ–∫—É–Ω–¥...")
                time.sleep(delay)
                delay *= 2
            else:
                raise RuntimeError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å —Ç–∞–±–ª–∏—Ü—É '{title}' –ø–æ—Å–ª–µ {retries} –ø–æ–ø—ã—Ç–æ–∫.")

async def get_funnel_v3(date_start: None, date_end: None, account: str, api_token: str):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –≤–æ—Ä–æ–Ω–∫–µ –ø—Ä–æ–¥–∞–∂ Wildberries"""
    products_list = []
    headers = {"Authorization": api_token}
    normal_delay = 2
    retry_delay = 20
    url = "https://seller-analytics-api.wildberries.ru/api/analytics/v3/sales-funnel/products"
    start = date_start
    end = date_end
    limit = 1000
    offset = 0
    max_attempts = 30
    attempt = 0
    semaphore = asyncio.Semaphore(10)
    
    async with semaphore:
        async with aiohttp.ClientSession(headers=headers) as session:
            while True:
                payload = {
                    "selectedPeriod": {
                        "start": start.strftime("%Y-%m-%d"),
                        "end": end.strftime("%Y-%m-%d")
                    },
                    "limit": limit,
                    "offset": offset
                }

                try:
                    async with session.post(url, json=payload) as res:
                        if res.status == 200:
                            data = await res.json()
                            products = data.get("data", {}).get("products", [])

                            if not products:
                                logging.info(f"üì≠ –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {account}")
                                break

                            for p in products:
                                p["account"] = account
                            products_list.extend(products)

                            logging.info(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(products_list)} —Ç–æ–≤–∞—Ä–æ–≤ ({len(products)} –Ω–æ–≤—ã—Ö) –¥–ª—è {account} –∑–∞ –ø–µ—Ä–∏–æ–¥ {payload['selectedPeriod']}")

                            if len(products) < limit:
                                break

                            offset += len(products)
                            attempt = 0
                            await asyncio.sleep(normal_delay)

                        elif res.status == 429:
                            logging.info(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ 429 –¥–ª—è {account}: —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤, –∂–¥–µ–º {retry_delay} —Å–µ–∫.")
                            await asyncio.sleep(retry_delay)
                            retry_delay += 0.1
                            attempt += 1
                            if attempt >= max_attempts:
                                logging.info(f"üö´ –ü—Ä–µ–≤—ã—à–µ–Ω–æ —á–∏—Å–ª–æ –ø–æ–ø—ã—Ç–æ–∫ ({max_attempts}) –¥–ª—è {account}")
                                break
                            continue

                        elif res.status in (400, 401, 403):
                            err = await res.json()
                            logging.info(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ {res.status} –¥–ª—è {account}: {err.get('detail', '–û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞')}")
                            return None

                        else:
                            logging.info(f"‚ö†Ô∏è –ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Å—Ç–∞—Ç—É—Å {res.status} –¥–ª—è {account}")
                            attempt += 1
                            if attempt >= max_attempts:
                                break

                except aiohttp.ClientError as err:
                    logging.info(f"üåê –°–µ—Ç–µ–≤–∞—è –æ—à–∏–±–∫–∞: {err}")
                    attempt += 1
                    if attempt >= max_attempts:
                        break

                except Exception as e:
                    logging.info(f"üí• –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")
                    break

        if products_list:
            logging.info(f"üü¢ –ó–∞–≤–µ—Ä—à–µ–Ω–æ –ø–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –ø–æ {account}. –í—Å–µ–≥–æ —Ç–æ–≤–∞—Ä–æ–≤: {len(products_list)}")
            return products_list
        else:
            logging.info(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –ø–æ –≤–æ—Ä–æ–Ω–∫–µ –ø—Ä–æ–¥–∞–∂ –¥–ª—è {account}")
            return None

async def fetch_all(date_start: int, date_end: None):
    # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–Ω–∏–∫ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –æ –ø–æ—Å—Ç–∞–≤–∫–∞—Ö –ø–æ –≤—Å–µ–º –∞–∫–∫–∞—É–Ω—Ç–∞–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ
    tasks = [get_funnel_v3(date_start, date_end, account, api_token) for account, api_token in load_api_tokens().items()]
    res = await asyncio.gather(*tasks)
    return res

# === –î–ª—è –µ–∂–µ–¥–Ω–µ–≤–Ω–æ–π –≤–æ—Ä–æ–Ω–∫–∏
def batchify(data, batch_size):
    """
    Splits data into batches of a specified size.

    Parameters:
    - data: The list of items to be batched.
    - batch_size: The size of each batch.

    Returns:
    - A generator yielding batches of data.
    """
    for i in range(0, len(data), batch_size):
        yield data[i:i + batch_size]

async def process_funnel_daily(days_count=1):
    """
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è: —Å–æ–±–∏—Ä–∞–µ–º –í–°–ï –¥–∞–Ω–Ω—ã–µ –≤ –æ–¥–∏–Ω DataFrame –∑–∞ 3 –º–µ—Å—è—Ü–∞
    """
    # === 1. –£–ö–ê–ó–´–í–ê–ï–ú –ù–£–ñ–ù–´–ô –ü–ï–†–ò–û–î –í  ===
    bath_size = 28
    date_ranges = []
    for day_num in range(1, days_count + 1):
        found_day = datetime.now()-timedelta(days=day_num)
        first_date, last_date = found_day, found_day
        date_ranges.append((first_date, last_date))
    
    print(f"üìÖ –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∑–∞ {len(date_ranges)} –¥–Ω–µ–π...")

    batches = batchify(date_ranges, bath_size)

    # === 2. –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–´–ô –ó–ê–ü–†–û–° –í–°–ï–• –ú–ï–°–Ø–¶–ï–í ===
    list_dfs = []
    for batch in batches:
        tasks = [fetch_all(first, last) for first, last in batch]
        results = await asyncio.gather(*tasks)
        
        print(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {sum(len(r) for r in results)} –∑–∞–ø–∏—Å–µ–π")
        
        # === 3. –û–ë–™–ï–î–ò–ù–Ø–ï–ú –í–°–ï –î–ê–ù–ù–´–ï –í –û–î–ò–ù –°–ü–ò–°–û–ö ===
        all_products = []
        for result in results:
            for acc_data in result:
                if acc_data:
                    all_products.extend(acc_data)
        
        print(f"üì¶ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(all_products)} —Ç–æ–≤–∞—Ä–æ–≤")
        
        # === 4. –û–ë–†–ê–ë–û–¢–ö–ê –í–°–ï–• –î–ê–ù–ù–´–• ===
        rows = []
        for product in all_products:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ 
            prod_info = product.get("product", {})
            stat = product.get("statistic", {})
            selected = stat.get("selected", {})
            time_to_ready = selected.get("timeToReady", {})
            
            # –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            row = {
                "account": product.get("account"),
                "nm_id": prod_info.get("nmId"),
                "vendor_code": prod_info.get("vendorCode"),  
                "title": prod_info.get("title"),
                "subject_id": prod_info.get("subjectId"),
                "subject_name": prod_info.get("subjectName"),
                "brand_name": prod_info.get("brandName"),
                "product_rating": prod_info.get("productRating"),
                "feedback_rating": prod_info.get("feedbackRating"),
                "stocks_wb": prod_info.get("stocks", {}).get("wb"),
                "stocks_mp": prod_info.get("stocks", {}).get("mp"),
                "balance_sum": prod_info.get("stocks", {}).get("balanceSum"),
            }
            
            # –ú–µ—Ç—Ä–∏–∫–∏ selected
            row.update({
                "open_count": selected.get("openCount"),
                "cart_count": selected.get("cartCount"),
                "order_count": selected.get("orderCount"),
                "orders_sum": selected.get("orderSum"),
                "buyout_count": selected.get("buyoutCount"),
                "buyout_sum": selected.get("buyoutSum"),
                "cancel_count": selected.get("cancelCount"),
                "cancel_sum": selected.get("cancelSum"),
                "avg_price": selected.get("avgPrice"),
                "avg_orders_count_per_day": selected.get("avgOrdersCountPerDay"),
                "share_order_percent": selected.get("shareOrderPercent"),
                "add_to_wish_list": selected.get("addToWishlist"),
                "time_to_ready": (
                    time_to_ready.get("days", 0) * 24 * 60 +
                    time_to_ready.get("hours", 0) * 60 +
                    time_to_ready.get("mins", 0)
                ),
                "localization_percent": selected.get("localizationPercent"),
                "date": selected.get("period", {}).get("end"),
            })
            
            rows.append(row)
                
        # === 5. –û–î–ò–ù DataFrame ===
        df_full = pd.DataFrame(rows)
        list_dfs.append(df_full)
    df_final = pd.concat(list_dfs)
    # === 6. –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ ===
    # df_final['month'] = pd.to_datetime(df_final['date']).dt.strftime('%m-%Y')
    # df_final['wild'] = df_final['vendor_code'].str.extract(r'(wild\d+)')
    
    print(f"‚ö° DataFrame —Å–æ–∑–¥–∞–Ω: {len(df_final)} —Å—Ç—Ä–æ–∫ –∑–∞ {len(date_ranges)} –¥–Ω–µ–π")   
    return df_final

def send_df_to_google(df, sheet):
    """
    –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç DataFrame –Ω–∞ —É–∫–∞–∑–∞–Ω–Ω—ã–π –ª–∏—Å—Ç Google –¢–∞–±–ª–∏—Ü—ã.

    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
    df (DataFrame): DataFrame, –∫–æ—Ç–æ—Ä—ã–π –Ω—É–∂–Ω–æ –æ—Ç–ø—Ä–∞–≤–∏—Ç—å.
    sheet (gspread.models.Worksheet): –û–±—ä–µ–∫—Ç –ª–∏—Å—Ç–∞, –Ω–∞ –∫–æ—Ç–æ—Ä—ã–π –±—É–¥—É—Ç –¥–æ–±–∞–≤–ª–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ.

    –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ:
    None
    """
    try:
        # –î–∞–Ω–Ω—ã–µ, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å
        df_data_to_append = [df.columns.values.tolist()] + df.values.tolist()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –ª–∏—Å—Ç–µ
        existing_data = sheet.get_all_values()
        
        if len(existing_data) <= 1:  # –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç
            print("–î–æ–±–∞–≤–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∏ –¥–∞–Ω–Ω—ã–µ")
            sheet.append_rows(df_data_to_append, value_input_option='USER_ENTERED')
             # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â—É—é –¥–∞—Ç—É –∏ –≤—Ä–µ–º—è
            now = datetime.now()
            formatted_time = now.strftime("%Y-%m-%d %H:%M:%S")
            
            # –ü–æ–ª—É—á–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–ª–æ–Ω–æ–∫ –Ω–∞ –ª–∏—Å—Ç–µ
            max_columns = sheet.col_count
            
            # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –¥–∞—Ç—É –∏ –≤—Ä–µ–º—è –≤ –ø–µ—Ä–≤—É—é —Å—Ç—Ä–æ–∫—É –ø–æ—Å–ª–µ–¥–Ω–µ–π –∫–æ–ª–æ–Ω–∫–∏
            sheet.update_cell(1, max_columns, formatted_time)
            print(f"–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è: {formatted_time}")
        else:
            print("–î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –¥–∞–Ω–Ω—ã–µ")
            sheet.append_rows(df_data_to_append[1:], value_input_option='USER_ENTERED')
            now = datetime.now()
            formatted_time = now.strftime("%Y-%m-%d %H:%M:%S")
            
            # –ü–æ–ª—É—á–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–ª–æ–Ω–æ–∫ –Ω–∞ –ª–∏—Å—Ç–µ
            max_columns = sheet.col_count
            
            # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –¥–∞—Ç—É –∏ –≤—Ä–µ–º—è –≤ –ø–µ—Ä–≤—É—é —Å—Ç—Ä–æ–∫—É –ø–æ—Å–ª–µ–¥–Ω–µ–π –∫–æ–ª–æ–Ω–∫–∏
            sheet.update_cell(1, max_columns, formatted_time)
            print(f"–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è: {formatted_time}")
            
    except Exception as e:
        print(f"An error occurred: {e}")

async def main_funnel_daily(days_count=1):
    df = await process_funnel_daily(days_count=days_count)
    df.drop_duplicates
    table = safe_open_spreadsheet("victoria_project")
    sheet = table.worksheet("–ë–î_–í–æ—Ä–æ–Ω–∫–∞")
    send_df_to_google(df, sheet)